{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94786456-39cd-4b51-b17c-bf06ed9348f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Resume Chatbot Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af74d9df-b595-4ab9-8c55-15f330733be1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-pdf-self-managed-0.png?raw=true\" style=\"width: 800px; margin-left: 10px\">\n",
    "\n",
    "### Steps:\n",
    "\n",
    "- Use autoloader to load the binary PDFs into our first table. \n",
    "- Use the `unstructured` library  to parse the text content of the PDFs.\n",
    "- Use `llama_index` or `Langchain` to split the texts into chunks.\n",
    "- Compute embeddings for the chunks.\n",
    "- Save our text chunks + embeddings in a Delta Lake table, ready for Vector Search indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e0dadf-c72b-4fda-929b-0416363671a0",
     "showTitle": false,
     "title": "Install required external libraries "
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.30.2 \"unstructured[pdf,docx]==0.10.30\" langchain==0.0.319 llama-index==0.9.3 databricks-vectorsearch==0.20 pydantic==1.10.9 mlflow==2.9.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d5d158-142d-4c3d-9f6e-bbb77ecd2fbf",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-init-advanced $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704892bd-3786-4525-96e9-218ab6f73686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "install_ocr_on_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c52ae15-cbc5-4b62-87d9-d5a861ba6066",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ingesting Resume PDFs\n",
    "1. Upload all the pdfs to [Unity Catalog Volume](https://docs.databricks.com/en/connect/unity-catalog/volumes.html)\n",
    "1. First, let's ingest our PDFs as a Delta Lake table with path urls and content in binary format\n",
    "1. We'll use [Databricks Autoloader](https://docs.databricks.com/en/ingestion/auto-loader/index.html) to incrementally ingest unstructured PDF data in binary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751024b8-0d92-4aaf-997e-35586470ae4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS volume_resume;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "799f5055-6fb0-4fd9-96c9-e13c079a1de1",
     "showTitle": false,
     "title": "Our pdf or docx files are available in our Volume (or DBFS)"
    }
   },
   "outputs": [],
   "source": [
    "# List our raw PDF docs\n",
    "volume_folder =  f\"/Volumes/{catalog}/{db}/volume_resume\"\n",
    "display(dbutils.fs.ls(volume_folder+\"/resume-pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b40e12e4-b0ca-4479-be6d-4a24228c5d32",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cleanup from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c203d1c-f3cd-45a0-ba35-b8e9ecc2ac5e",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS dbdemos_vishesh.llm_db.pdf_raw;\n",
    "DROP TABLE IF EXISTS dbdemos_vishesh.llm_db.resume_pdf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be21bd96-90e0-4260-82b6-53fa391028b3",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(f'dbfs:{volume_folder}/checkpoints/raw_docs', True)\n",
    "dbutils.fs.rm(f\"dbfs:{volume_folder}/checkpoints/pdf_chunk\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9d83b6-6a4f-42df-9697-476fc29fae15",
     "showTitle": true,
     "title": "Ingesting PDF files as binary format using Databricks cloudFiles (Autoloader)"
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream.format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'BINARYFILE')\n",
    "        .option(\"pathGlobFilter\", \"*.pdf\")\n",
    "        .load('dbfs:'+volume_folder+\"/resume-pdf\"))\n",
    "\n",
    "# Write the data as a Delta table\n",
    "(df.writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/raw_docs')\n",
    "  .table('pdf_raw').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "538405b7-f8fe-4c0e-ba2b-c51cb368e4e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM pdf_raw LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e11f366-dda4-4e84-9349-02c764b7a94b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/chunk-window-size.png?raw=true\" style=\"float: right\" width=\"600px\">\n",
    "\n",
    "## Extracting our PDF content and splitting into smaller chunks\n",
    "1. We need to convert the PDF documents bytes to text, and extract chunks from their content\n",
    "1. We'll extract the content and then use llama_index `SentenceSplitter`, and ensure that each chunk isn't bigger than 500 tokens. Prompt+Answer should stay below your model max window size (4096 for llama2)\n",
    "1. The chunk size and chunk overlap depend on the use case and the dataset. Review chunks to ensure they make sense and contain relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104efd37-e9ca-4c52-8c3e-ebb9a2765b29",
     "showTitle": true,
     "title": "Transform pdf as text"
    }
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "import re\n",
    "\n",
    "def extract_doc_text(x : bytes) -> str:\n",
    "  # Read files and extract the values with unstructured\n",
    "  sections = partition(file=io.BytesIO(x))\n",
    "  def clean_section(txt):\n",
    "    txt = re.sub(r'\\n', '', txt)\n",
    "    return re.sub(r' ?\\.', '.', txt)\n",
    "  # Default split is by section of document, concatenate them all together because we want to split by sentence instead.\n",
    "  return \"\\n\".join([clean_section(s.text) for s in sections]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "456edb41-a7af-42f4-b07e-6d5f5dd0d7ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_resume_attributes(prompt, endpoint_name = 'dbdemos-azure-openai'):\n",
    "  host = 'e2-demo-field-eng.cloud.databricks.com'\n",
    "  token = ''\n",
    "  url = f\"https://{host}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "\n",
    "  payload = json.dumps({\n",
    "    \"messages\": [{\"role\": \"user\",\n",
    "       \"content\": prompt\n",
    "       }],\n",
    "    \"max_tokens\": 500})\n",
    "  headers = {'Content-Type': 'application/json',\n",
    "             'Authorization': f'Bearer {token}'}\n",
    "  \n",
    "  response = requests.request(\"POST\", url, headers=headers, data=payload).json()\n",
    "  return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6178fb5c-5036-4430-b35c-a45184dba290",
     "showTitle": true,
     "title": "Trying text extraction function with a single pdf file"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "with open('/Volumes/dbdemos_vishesh/llm_db/volume_resume/resume-pdf/11911050 Nidhi.pdf', 'rb') as pdf:\n",
    "  pdf_content = pdf.read()\n",
    "  doc = extract_doc_text(pdf_content)\n",
    "\n",
    "  prompt = f\"\"\"Please summarize the following information relevant for hiring and recruitment from the provided resume text : ```{doc}```. Make sure you have included List all the skills relevant to Data Science and Machine Learning, graduation year or expected graduation year (if not available, set current year), and provide a summary of the candidate's work experience, including job titles, company names, and dates of employment.\"\"\"\n",
    "  \n",
    "  # prompt = f\"\"\"Please extract the following information from the provided resume text and format it as JSON: ```{doc}```. The json format should look like: {{\"skills\": \"List all relevant skillsets mentioned in the resume.\", \"graduation_year\": \"Extract the graduation year or expected graduation year (if not available, set current year)\", \"work_experience\": \"Provide a summary of the candidate's work experience, including job titles, company names, and dates of employment.\"}}\"\"\"\n",
    "  \n",
    "  res_sum = get_resume_attributes(prompt)\n",
    "  print(\"PDF Text:\\r\\n\")\n",
    "  print(doc)\n",
    "  print(\"\\r\\nPDF Summary:\\r\\n\")\n",
    "  print(res_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b08766a-7991-4d20-acd4-9dc9ab20fe62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)\n",
    "    set_global_tokenizer(\n",
    "      AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    def extract_and_split(b):\n",
    "      txt = extract_doc_text(b)\n",
    "      \n",
    "      prompt = f\"\"\"Please summarize the following information relevant for hiring and recruitment from the provided resume text : ```{doc}```. Make sure you have included List all the skills relevant to Data Science and Machine Learning, graduation year or expected graduation year (if not available, set current year), and provide a summary of the candidate's work experience, including job titles, company names, and dates of employment.\"\"\"\n",
    "      \n",
    "      # prompt = f\"\"\"Please extract the following information from the provided resume text and format it as JSON: ```{doc}```. The json format should look like: {{\"skills\": \"List all relevant skillsets mentioned in the resume.\", \"graduation_year\": \"Extract the graduation year or expected graduation year (if not available, set current year)\", \"work_experience\": \"Provide a summary of the candidate's work experience, including job titles, company names, and dates of employment.\"}}\"\"\"\n",
    "      \n",
    "      res_sum = get_resume_attributes(prompt)\n",
    "\n",
    "      nodes = splitter.get_nodes_from_documents([Document(text=res_sum)])\n",
    "      return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6c8664b-19ab-4509-beee-59f187172921",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/databricks-vector-search-type.png?raw=true\" style=\"float: right\" width=\"800px\">\n",
    "\n",
    "###Databricks provide multiple types of vector search indexes:\n",
    "\n",
    "- **Managed embeddings**: you provide a text column and endpoint name and Databricks synchronizes the index with your Delta table \n",
    "- **Self Managed embeddings**: you compute the embeddings and save them as a field of your Delta Table, Databricks will then synchronize the index\n",
    "- **Direct index**: when you want to use and update the index without having a Delta Table.\n",
    "\n",
    "In this demo, we will setup a **Self-managed Embeddings** index by computing the embeddings of our chunks and saving them as a Delta Lake table field as `array&ltfloat&gt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20abbbb7-48ce-4ce9-a045-bcea0c441f5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "###Databricks supports several endpoint types to compute embeddings or evaluate a model:\n",
    "- A **foundation model endpoint**, provided by databricks (ex: llama2-70B, MPT...)\n",
    "- An **external endpoint**, acting as a gateway to an external model (ex: Azure OpenAI)\n",
    "- A **custom**, fined-tuned model hosted on Databricks model service\n",
    "\n",
    "See [Model Serving Endpoint page](/ml/endpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f859064-3b3d-414c-bbe9-d6167cc20371",
     "showTitle": false,
     "title": "Using Databricks Foundation model BGE as embedding endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# bge-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "## NOTE: if you change your embedding model here, make sure you change it in the query step too\n",
    "embeddings = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [\"Who has python skills?\"]})\n",
    "pprint(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ee4a16-f1b6-4eea-9af6-97c52b675133",
     "showTitle": false,
     "title": "Create the final resume_pdf table containing chunks and embeddings"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS resume_pdf (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36d480a3-c5a5-43c3-918e-66259b2ce83b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Computing the chunk embeddings and saving them to our Delta Table\n",
    "\n",
    "1. The last step is to now compute an embedding for all our chunks using the foundation model endpoint\n",
    "2. Note that this part would typically be setup as a production-grade job, running as soon as a new documentation page is updated (incremental DLT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e0b243-4165-4a3f-8332-85e1a33e7792",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    def get_embeddings(batch):\n",
    "        #Note: this will fail if an exception is thrown during embedding creation (add try/except if needed) \n",
    "        response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": batch})\n",
    "        return [e['embedding'] for e in response.data]\n",
    "\n",
    "    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    # Process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "    return pd.Series(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a5e38c8-2fa3-4781-8dda-de03aeb6d80f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream.table('pdf_raw')\n",
    "      .withColumn(\"content\", F.explode(read_as_chunk(\"content\")))\n",
    "      .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "      .selectExpr('path as url', 'content', 'embedding')\n",
    "  .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/pdf_chunk')\n",
    "    .table('resume_pdf').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc80cc81-4af3-469c-a0b2-9686c36ec3d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM resume_pdf WHERE url like '%.pdf' limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b33b48e0-58e4-4f5c-a726-a120fc8bcdac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Creating Self-Managed Vector Search Index\n",
    "\n",
    "1. we'll configure Databricks Vector Search endpoint and a vector search index using this endpoint on top of our delta table\n",
    "1. Vector search index uses a Vector search endpoint to serve the embeddings (Vector Search API endpoint)\n",
    "1. Multiple Indexes can use the same endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56e9530c-05ec-442c-b978-0787b0c328b6",
     "showTitle": false,
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "if VECTOR_SEARCH_ENDPOINT_NAME not in [e['name'] for e in vsc.list_endpoints()['endpoints']]:\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "939619be-3e2d-4871-b11c-6feed9420baa",
     "showTitle": false,
     "title": "Create the Self-managed vector search using our endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.resume_pdf\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.resume_pdf_self_managed_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\", #Sync needs to be manually triggered\n",
    "    primary_key=\"id\",\n",
    "    embedding_dimension=1024, #Match your model embedding size (bge)\n",
    "    embedding_vector_column=\"embedding\"\n",
    "  )\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "#Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2254959f-f079-436e-b3f1-627deab0c876",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Searching for similar content\n",
    "\n",
    "1. Databricks will automatically capture and synchronize new entries in your Delta Lake Table\n",
    "1. `similarity_search` also supports a filters parameter. This is useful to add a security layer to your RAG system: you can filter out some sensitive content based on who is doing the call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a8f3b2-ed6e-421d-933f-d5a80019b256",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"\"\"Search for resumes of candidates with experience in building and maintaining Machine Learning solutions at scale, proficient in predictive modeling, Machine Learning, Deep Learning, Reinforcement Learning. I am looking for skills including pandas, numpy, sql, python, machine learning, data science\"\"\"\n",
    "# question = \"\"\"Search for resumes of candidates with experience in building and maintaining ML solutions at scale, proficient in predictive modeling (ML, DL/RL) and experimentation/causal inference methods. They should have experience working as a data scientist. Look for candidates with a proven track record of designing end-to-end ML systems capable of handling large-scale operations or working on Kaggle datasets as part of their projects or internships\"\"\"\n",
    "# question = \"\"\"{\n",
    "#   \"criteria\": {\n",
    "#     \"skills\": [\"Machine Learning\", \"Deep Learning\", \"SQL\", \"Python\", \"Pandas\", \"Spark\", \"MLOps\", \"Data Science\", \"Statistics\"],\n",
    "#     \"graduation_year\": \"2023\",\n",
    "#     \"work_experience\": \"internship or job\",\n",
    "#     \"additional_requirements\": [\n",
    "#       \"data scientist\",\n",
    "#       \"proven track record of designing end-to-end ML systems capable of handling large-scale operations\",\n",
    "#       \"experience with Kaggle datasets as part of projects or internships\"\n",
    "#     ]\n",
    "#   }\"\"\"\n",
    "\n",
    "response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [question]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_vector=embeddings[0],\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=5)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "pprint(docs)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4161359592084653,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "resume-chatbot-demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
